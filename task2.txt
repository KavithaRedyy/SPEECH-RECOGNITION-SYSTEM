Option 1: Using the SpeechRecognition Library (Easy & Popular)
This uses Google Web Speech API (online) to transcribe audio.

Step 1: Install the library

bash
Copy
Edit
pip install SpeechRecognition
Step 2: Create a Python script, e.g., speech_to_text.py:

python
Copy
Edit
import speech_recognition as sr

def transcribe_audio(file_path):
    # Initialize recognizer
    r = sr.Recognizer()

    # Load audio file
    with sr.AudioFile(file_path) as source:
        audio_data = r.record(source)

    # Recognize (convert from speech to text)
    try:
        text = r.recognize_google(audio_data)
        return text
    except sr.UnknownValueError:
        return "Could not understand the audio."
    except sr.RequestError:
        return "Could not request results; check your internet connection."

if __name__ == "__main__":
    file_path = "audio.wav"  # Replace with your audio file path (WAV format)
    transcription = transcribe_audio(file_path)
    print("Transcription:", transcription)
Note:

Input audio must be a WAV file (mono, 16-bit PCM, 16000 Hz recommended).

Works online because it sends audio to Googleâ€™s API.

Option 2: Using Wav2Vec 2.0 (Offline, Hugging Face Transformer model)
More advanced, runs offline with PyTorch and pre-trained Wav2Vec2 model.

Step 1: Install necessary packages

bash
Copy
Edit
pip install transformers torchaudio soundfile
Step 2: Python script:

python
Copy
Edit
import torch
import torchaudio
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor

def transcribe_audio_wav2vec(file_path):
    # Load pretrained model and processor
    processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
    model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")

    # Load audio
    speech, sample_rate = torchaudio.load(file_path)
    if sample_rate != 16000:
        resampler = torchaudio.transforms.Resample(sample_rate, 16000)
        speech = resampler(speech)

    input_values = processor(speech.squeeze().numpy(), return_tensors="pt", sampling_rate=16000).input_values

    # Perform inference
    with torch.no_grad():
        logits = model(input_values).logits

    predicted_ids = torch.argmax(logits, dim=-1)
    transcription = processor.decode(predicted_ids[0])
    return transcription.lower()

if __name__ == "__main__":
    file_path = "audio.wav"  # Provide your audio file path here
    result = transcribe_audio_wav2vec(file_path)
    print("Transcription:", result)
